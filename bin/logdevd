#!/usr/bin/python

import sys
import optparse
import logdevd
import signal
import json
import logging
import yaml

#-----------------------------------------------------------------------------
# command line options {{{

parser = optparse.OptionParser(
    usage = "%prog [options]"
)

parser.add_option(
    "-c", "--config", dest = "config",
    default = "/etc/logdevourer/logdevourer.conf",
    help = "configuration file", metavar = "FILE",
)
parser.add_option(
    "-l", "--logging", dest = "logging_config",
    default = None,
    help = "logging configuration file", metavar = "FILE",
)
parser.add_option(
    "-s", "--state-dir", dest = "state_dir",
    default = "/var/lib/logdevourer",
    help = "PID file for going daemon", metavar = "FILE",
)
parser.add_option(
    "-p", "--pid-file", dest = "pid_file", default = None,
    help = "PID file for going daemon", metavar = "FILE",
)
parser.add_option(
    "-i", "--stdio", dest = "stdio_only",
    action = "store_true", default = False,
    help = "read from STDIN, write to STDOUT, ignoring any sources and"
           " destinations from configuration file"
           " (--state-dir is also not used)",
)
parser.add_option(
    "-d", "--daemon", dest = "daemonize",
    action = "store_true", default = False,
    help = "detach from terminal (run as a daemon)",
)
parser.add_option(
    "-u", "--user", dest = "user", default = None,
    help = "user to run as",
)
parser.add_option(
    "-g", "--group", dest = "group", default = None,
    help = "group to run as",
)

(options, args) = parser.parse_args()

# }}}
#-----------------------------------------------------------------------------
# configure logging {{{

if options.logging_config is not None:
    with open(options.logging_config) as cf:
        logging_config = yaml.safe_load(cf)
elif options.stdio_only:
    logging_config = {
        "version": 1,
        "root": {
            "level": "NOTSET",
            "handlers": ["sink"],
        },
        "handlers": {
            "sink": { "class": "logdevd.logging_handlers.NullHandler" },
        },
    }
else:
    logging_config = {
        "version": 1,
        "root": {
            "level": "NOTSET",
            "handlers": ["syslog"],
        },
        "formatters": {
            "syslog_formatter": {
                "format": "[%(name)s] %(message)s",
            },
        },
        "handlers": {
            "syslog": {
                "class": "logdevd.logging_handlers.SysLogHandler",
                "formatter": "syslog_formatter",
                "facility": "syslog",
                "process_name": "logdevd",
            },
        },
    }

try:
    from logging.config import dictConfig
except ImportError:
    # Python 2.6
    from logdevd.logging_config import dictConfig
dictConfig(logging_config)

# }}}
#-----------------------------------------------------------------------------
# main loop, along with state variables and options {{{

class Daemon:
    def __init__(self, config, state_dir, stdio_only = False):
        self.config = config
        self.stdio_only = stdio_only
        self.log_unparsed = False
        self.send_unparsed = True
        self.state_dir = state_dir
        self.poll_h = None
        # special list for sources that are plain files, so they're not
        # polled (at EOF poll immediately returns "ready to read")
        self.unpollable_opened_sources = []
        self.sources = []
        self.destinations = []
        self.lognorm = None
        # TODO: raise exception on error (no previous config to fall back to)
        self.reload()

    def encode_json(self, struct):
        return json.dumps(struct, sort_keys = True)

    def monitor_source(self, source):
        if source.poll_makes_sense():
            self.poll_h.add(source)
        else:
            self.unpollable_opened_sources.append(source)

    def unmonitor_source(self, source):
        self.poll_h.remove(source)
        try:
            self.unpollable_opened_sources.remove(source)
        except ValueError:
            pass # it's OK if `source' is not in this list

    def reload(self):
        logger = logging.getLogger("configuration")
        # before replacing sources, inform those that they might want to flush
        # their (buffered) state to disk
        if len(self.sources) > 0:
            logger.info("flushing buffers in all sources")
        for source in self.sources:
            source.flush()
            source.close()
        del self.unpollable_opened_sources[:]
        self.poll_h = logdevd.poll.Poll()
        # TODO: try-catch
        logger.info("loading config file %s", self.config)
        cfg = logdevd.config.load(self.config, self.state_dir, self.stdio_only)
        # TODO: convergence
        (self.sources, self.destinations, self.lognorm, config) = cfg
        for source in self.sources:
            if not source.is_opened():
                source.open()
            if not source.is_opened():
                continue
            logger.info("added source %s", source)
            self.monitor_source(source)
        self.send_unparsed = config["options"].get("send_unparsed", True)
        self.log_unparsed  = config["options"].get("log_unparsed", False)

    def reopen_sources_if_necessary(self):
        # XXX: when logging, remember that this method is called every 500ms,
        # so if adding the source has not succeeded (e.g. file is still
        # missing), it would spam logfile with meaningless entries
        logger = logging.getLogger("sources")
        for source in self.sources:
            # if it's not opened, it's not present in self.poll_h nor in
            # self.unpollable_opened_sources
            if not source.is_opened():
                # add missing log sources
                source.open()
                if source.is_opened():
                    logger.info("added source %s", source)
                    self.monitor_source(source)
            elif source.reopen_necessary():
                self.unmonitor_source(source)
                # reopen files that need it
                source.reopen()
                if source.is_opened():
                    logger.info("reopened source %s", source)
                    self.monitor_source(source)
                else:
                    logger.info("closed source %s", source)

    def normalize(self, log_line):
        result = self.lognorm.normalize(log_line)
        # XXX: "*" field can be either a string containing JSON hash or
        # an already parsed dictionary; this is to support liblognorm both
        # 1.1.1 and later versions, with "json" field type being introduced in
        # 1.1.2
        if "*" in result:
            json_field = result.pop("*")
            old_result = result
            try:
                if isinstance(json_field, dict):
                    result = json_field
                else: # should be string
                    result = json.loads(json_field)
                result.update(old_result)
            except:
                result = {
                    "originalmsg": log_line,
                    "unparsed-data": json_field,
                }
        # remains after parsing the log line
        if "originalmsg" in result and "unparsed-data" in result:
            if self.log_unparsed:
                logger = logging.getLogger("normalize")
                logger.info("unparsed log entry: %s", self.encode_json(result))
            if not self.send_unparsed:
                return None
        return result

    def fan_out(self, message):
        line = self.encode_json(message)
        for d in self.destinations:
            d.send(line)

    def filecount(self):
        return self.poll_h.count()

    def poll(self, timeout):
        return self.poll_h.poll(timeout)

    def sighandler(self, signum, stack_frame):
        logger = logging.getLogger("signal")
        if signum == signal.SIGHUP:
            logger.info("received SIGHUP")
            self.reload()
        elif signum == signal.SIGTERM or signum == signal.SIGINT:
            logger.info("received signal; terminating")
            sys.exit()
        else:
            logger.info("received signal %d; ignoring", signum)

# }}}
#-----------------------------------------------------------------------------

logger = logging.getLogger()

logger.info("preparing daemon's state")
daemon = Daemon(options.config, options.state_dir, options.stdio_only)

signal.signal(signal.SIGHUP, daemon.sighandler)
signal.signal(signal.SIGINT, daemon.sighandler)
signal.signal(signal.SIGTERM, daemon.sighandler)

#-----------------------------------------------------------------------------
# daemonization {{{

pid_file = logdevd.daemonize.PidFile(options.pid_file) # None is OK

logdevd.daemonize.setguid(options.user, options.group)

if options.daemonize:
    result = logdevd.daemonize.detach("/")
    if result == logdevd.daemonize.PARENT:
        sys.exit(0)
    logger.info("detached from terminal")
    # else: result == logdevd.daemonize.CHILD
    # errors result in an exception
    pid_file.update()

# set remove-on-close flag
pid_file.claim()

# }}}
#-----------------------------------------------------------------------------

logger.info("entering read-parse-send loop")
while daemon.filecount() > 0 or not options.stdio_only:
    # check every 250ms for sources that need reopening
    canread = daemon.poll(250)
    for source in canread + daemon.unpollable_opened_sources:
        for line in source.try_readlines():
            message = daemon.normalize(line)
            if message is not None:
                daemon.fan_out(message)
    daemon.reopen_sources_if_necessary()

#-----------------------------------------------------------------------------
# vim:ft=python:foldmethod=marker
